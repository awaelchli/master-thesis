\chapter{Conclusion and Future Work}

The thesis presented a deep learning model for Visual Odometry that makes use of recurrent connections in order to provide a context for each time step where camera location and orientation is estimated.
The trained neural network can handle videos of arbitrary length and reconstructs the camera path in real-time.
An analysis on real and synthetic data showed that training RNNs for the domain of videos is challenging.
We have explored a variety of training schemes on small and large datasets.
Sequence overlap, dropout and continuously increasing the sequence length (BPTT) seem to be effective on the small KITTI dataset.
Furthermore, the thesis uncovers the problem with global pose and overfitting to the length of training sequences.
Although we have solved this problem by converting the camera poses to incremental motions, the situation is rather confusing and counter-intuitive.
At this point, we can only speculate on why the RNN does not generalize in a way that the camera pose is accumulated from the beginning of the video and over an arbitrary length.
The fact that replacing the LSTM with a affine layer results in better performance indicates that the recurrent state might not be used to the benefit of pose regression.
%, and this leaves the question why it does not simply get ignored in this case.

These issues require further investigation and give plenty of room for future work.
Estimating both global- and incremental pose at the same time would be a simple modification worth investigating.
In terms of limitations, the loss function requires a manual balance between rotational- and positional error.
The thesis did not explicitly explore the impact of the balance weight, but a solution that eliminates or automatically learns such a parameter could certainly have an impact on the convergence speed.

It would also be interesting to explore VO for stereo video input, as it could reduce the drift over time by providing double the amount of measurements for a single viewpoint.
This would of course require a suitable dataset and potentially increase the memory footprint.

Another possibility is to incorporate the new and improved FlowNet2 by~\cite{ilg2016flownet}.
The recently released PyTorch code base for the updated FlowNet could be integrated into the existing pipeline developed for this thesis.
Although the initialization of the CNN with weights from FlowNet help the convergence speed, it is unclear how large of a benefit the new FlowNet2 weights will bring, since only the first ten layers were used and the decoder part was removed all together. 

Lastly, there is room for improvement of the code base developed for this thesis.
For example, the currently used library for conversion between different pose representations (Euler, quaternion, etc.) does not have a GPU back-end.
It is therefore inefficient to convert poses on-the-fly as they have to be copied to CPU memory and back to the GPU.
The few functions for pose conversion could be re-implemented with PyTorch, as it provides a the necessary GPU support for its tensor operations.

\todo{Dataset with object motion annotations\\}
\todo{Stereo\\}
\todo{Pre-processing of dataset, different motion separated\\}
\todo{Different frame rate / speed in dataset\\}