\chapter{Introduction}
	
	\begin{figure}
		\centering
		\includegraphics[width=0.3\linewidth]{example-image-a}
		\caption{\label{fig:sfm-example}}
	\end{figure}
		
	\section{Motivation}
		
		Building and programming a robot that interacts with and learns from its environment is challenging.
		Usually the robot is designed to solve a specific task that requires some form of interaction with objects and the environment they occupy.
		Some robots require navigation to complete their task, e.g., a self-driving car that has to maneuver into a parking space.
		The robot has to determine its position and orientation in the world with respect to some coordinate system.
		Furthermore, it may be required to detect and prevent imminent collisions, which requires sensing the distance to nearby objects.
		One possible way to tackle the navigation problem is to use one or more cameras mounted to the body of the robot.
		In order for the robot to determine its location, it has to intelligently analyze the motion in the input image sequence coming from the video camera.
		To avoid collisions or allow interaction, the 3D structure of the surrounding world and objects has to be reconstructed from the camera input or measured by other sensors.
		
		\emph{Visual Odometry (VO)} \todo{reference} addresses the problem of recovering the ego-motion from a video input in real-time. 
		\emph{Structure from Motion (SfM)} \todo{reference}, an extension of VO, additionally recovers the 3D structure from the video input.
		Although these methods have many practical applications not only in robotics, they have some limitations.
		First of all, both VO and SfM rely on robust feature detection and matching between images in order to accurately estimate motion.
		The algorithms to extract these informations from images are hand-designed and tuned to work well in specific domains and environments.
		They often rely on strong assumptions about the observed scene, lighting conditions or type of motions involved.
		These assumptions are made to avoid possible ambiguities arising from, e.g., dynamic motion in the scene or specular reflections.
		Furthermore, classic VO and SfM do not leverage high-level information about the image content that could be used to eliminate and resolve these conflicts.
		
		\todo{Better Transition}
		Very quickly after birth, we humans learn to develop a complex understanding of the three-dimensional world around us. 
		The emphasis here is on \emph{learning}.
		There is no external force or teacher that predetermines how the network of neurons in our brain develops and grows.
		It is the continuous loop of information retrieval (e.g. seeing, feeling) and action (e.g. body movement) that determines reward or penalty (e.g. walking or falling) which then influences the development of connections in the brain.
		\todo{Example removing vision of newborn}
		Recent advances in Deep Learning have shown that machines are able to learn and identify patterns in data and use them to solve complex problems that also humans are faced with everyday, e.g., recognizing a persons face or solving puzzle games.
		Deep Learning has had a significant impact on Computer Vision and is under rapid development, becoming more popular every year.
		
		The potential and recent successes of Deep Learning motivate an alternative approach to the classic VO and SfM algorithms.
		Learning a powerful representation of camera motion from a large dataset of image sequences could help overcome some of the aforementioned challenges in SfM as well as reduce the computational cost to provide real-time mobile applications.
		
		
		
		
		%Among others, the visual input from one or more cameras is 
		%There are many sensors that can be employed to make navigation possible. 
		%A robot can be built in form of a human, a car or a flying drone.
		%In order to do so, it has one or more sensors, such as cameras
		
%		The term \emph{structure from motion} refers to the observation of 2D motion in a series of images that can be used to recover 3D information.
%		The camera motion and 3D geometry are by itself completely independent components, but the observed images are only fully determined by the combination of the two.
%		%The motion of the camera has to be determined simultaneously with structure, because only both components together fully determine the observed projections.
%		When the desire is to only reconstruct the 3D, e.g. obtaining a 3D model of a vase, the camera parameters can be seen as a by-product of the SfM algorithm. 
%		After the point cloud is recovered, new views can be rendered on the fly without the need of the original camera poses.
%		On the other hand, when the interest lies in only recovering the camera path, then SfM produces the point cloud as a by-product.
%		In SfM, camera parameters and 3D are solved jointly in an iterative manner.
%		This process has a high computational cost.
%		It is often not suited for real-time mobile applications where reconstruction from hundreds and thousands of video frames is desired.
%		Another limitation is that SfM requires the observed scene to be static.
%		It means that the motion observed in the images can only come from the camera, and not from moving objects in the scene.
%		Otherwise, the reconstruction problem is ambiguous.
		
		%In practice, SfM algorithms are designed and tuned to work optimally in their application domain.
		%This requires a significant amount of manual intervention and prior knowledge.
		
		
		
		
		
		%We try to precisely describe our ever-changing world we live in. 
		
		
	\section{Visual Odometry by Deep Learning}
		% A model with deep learning (supervised)
		% Mapping sequences to sequences
		% Learning from the past: Recurrence
		
		%The interest of this thesis lies in recovering the camera parameters, i.e., the position and orientation.
		The problem of Visual Odometry is simple to formulate.
		Given a sequence of temporally ordered images, i.e. a video, the objective is to compute the camera pose at each frame.
		The camera pose is the location and orientation of the camera in the world coordinate system.
		It describes the change of coordinates from the local- to the world reference frame in form of a rotation and translation, each having three degrees of freedom.
		Figure~\ref{fig:overview-visual-odometry} shows an overview of the VO problem.
		\begin{figure}[t]
			\centering
			\includegraphics[width=\linewidth]{"Introduction/Problem Overview/overview"}
			\caption[Description of the Visual Odometry problem]
					{Overview of the Visual Odometry problem.
					 The input video (left) is mapped to a sequence of poses (right) describing the path of the camera.
					 \label{fig:overview-visual-odometry}}
		\end{figure}
		
		%Without loss of generality it is assumed that the world coordinate system coincides with reference frame of the first image in the sequence.
		There are three main problems that need to be addressed by the proposed solution for VO.
		First, one needs to specify a suitable reference frame and representation for the camera poses.
		Second, the system has to be designed in a way that it can handle a video input of variable length and frame rate.
		Third, it should be robust to noise in form of scene dynamics, lighting changes, camera shake or blur.
		Furthermore, the system must support any camera model without the need for manual calibration.
		
		In this thesis, we explore a Deep Learning approach to the VO problem.
		It involves training an artificial neural network on a large number of videos in a supervised fashion.
		Supervised learning requires that the true camera poses are known during the learning phase.
		% for continuous performance evaluation.
		In order for the network to learn a useful mapping from input to output, it is crucial that the collected data has accurate pose labels for each video frame.
		The first two of the aforementioned problems are addressed by the architectural choices for the neural network, and the robustness depends, for the most part, on the quality and quantity of the available data for learning.
		
		
	\section{Challenges}
		% Getting the sequences wth ground truth pose (using gps, IMU, synthetic)
		% 
		
		Training a neural network requires a large dataset with realistic data.
		Collecting the video data is time consuming, but the challenge lies in the collection of the ground truth, i.e., the camera pose.
		The accelerometers and gyroscopes in consumer cameras are not accurate enough to capture the pose of the device over a long period of time as they experience large measurement drift.
		GPS may be used to determine the horizontal location within a few meters depending on the signal strength, but this makes it impossible for indoor navigation.
		An alternative way to obtain precise camera pose is by generating it synthetically.
		However, the challenge here is to generate photo-realistic data with much variation.
	
	\section{Contribution}
		% mention the deepVO paper, based on this work
		% supplemental insight, reverse engineer
		% large and diverse synthetic dataset, as an extension to VIPER?
		%
	
	\section{Structure of the Thesis}
		The remainder of this thesis is structured as follows:
		\begin{itemize}
			\item Chapter 2 presents the prior works done on Structure from Motion and Visual Odometry, both the classical methods as well as the recent approaches with Deep Learning.
			
			\item Chapter 3 contains all the mathematical background needed for the rest of the thesis.
			Moreover, it reviews the concepts of machine learning, feedforward- and recurrent neural networks.
			
			\item Chapter 4 is a detailed description of the proposed deep learning model for Visual Odometry.
			
			\item Chapter 5 documents the experiments and results on real and synthetic datasets.
			It also shows the strengths and weaknesses of the system.
			
			\item Chapter 6 concludes the thesis with a discussion of the results and improvements for future work.
		\end{itemize}
		\todo{check again if up to date}