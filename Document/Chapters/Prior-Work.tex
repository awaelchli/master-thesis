\chapter{Prior Work}
	\todo{Typeset vectors/matrices with custom commands}
	
	Since the beginning, we humans have examined the world around us. 
	There is a certain urge to explore, build and shape the environment we live in.
	One thing that we have learned long ago is that every object, place, person changes its shape and appearance, the way it behaves and feels like. Sooner or later, everything ages, breaks and vanishes as it contributes to the increase of entropy in our universe.
	Often we wish to stop this process and capture a moment in time, forever and unchanged, so that we can go back and re-experience.
	Of course, with the technology we have today it is impossible to instantly capture and store every piece of information about a place or an object on a subatomic level.
	The complexity of such a measurement process is simply too high.
	However, we do have devices that allow us to record a sparse subset of all the information.
	One such device is the digital camera.
	It captures the light in the same fashion as the human eye.
	The camera has an image sensor that accumulates the energy from light rays entering through the aperture.
	The image that forms on the sensor is a perspective projection of the 3D world onto a 2D plane.
	We humans have two eyes that allow us to recover depth information by exploiting the displacement in the 2D projections of each eye.
	The same applies to a pair of cameras displaced along a horizontal baseline.
	A single 3D point in front of the cameras projects to a different 2D location on the two sensors.
	The resulting shift is called \emph{disparity}.
	It is inversely proportional to depth, i.e., a small disparity is the result of a point far away from the camera.
	Therefore, a stereo image pair allows us to get the depth values for each pixel.
	
	Although the depth reconstruction from stereo vision has many practical applications, it does not give a full 3D geometry of the observed scene, since a camera can only capture the visible parts.
	It is therefore not possible to reconstruct the 3D of surfaces that are occluded or outside the field of view. 
	In order to obtain a full 3D reconstruction of a static object or scene, images have to be taken from many positions and angles.
	Through the motion of the camera, in a continuous fashion new 2D measurements of points are obtained that would otherwise be occluded from a single viewing direction.
	The technique of recovering the 3D geometry from multiple images is called \emph{Structure from Motion (SfM)}.
	In the typical SfM pipeline the input is an ordered or unordered list of images and the output consists of the camera poses and the 3D point cloud.
	The typical SfM pipeline can be summarized in three basic steps:
	\begin{enumerate}
		\item Feature detection, extraction and matching
		\item Camera motion estimation
		\item Recovery of the 3D structure
	\end{enumerate}
	This chapter presents the prior work that focuses on one or multiple of these steps.
	Although the topic of this thesis is Visual Odometry, the discussion of prior works includes recovery of 3D structure to give a better overview since the two research areas are closely related. 
	
%	\noindent
%	Possible definition of structure from motion by \cite{survey2017}:
%	
%	\say{The structure from motion (SfM) problem in computer vision is the problem of recovering the three-dimensional (3D) structure of a stationary scene from a set of projective measurements, represented as a collection of two-dimensional (2D) images, via estimation of motion of the cameras corresponding to these images.}
%	
%	One can define the three basic steps of SfM as:
%	\begin{enumerate}
%		\item Feature detection, extraction and matching
%		\item Camera motion estimation
%		\item Recovery of 3D structure
%	\end{enumerate}

	\section{Classic Methods}
	
		\subsection{Feature Extraction and Matching}
			% methods: SURF, ORB, SIFT
			%The Eight Point Algorithm is not the only method that requires accurate correspondences across image pairs.
			All SfM algorithms depend on a robust feature detection and matching to solve the correspondence problem between two or more images.
			It is usually the first step in the SfM pipeline.
			A pair of 3D coordinates, one in each camera view, is called \emph{corresponding} if they depict the projection of the same 3D point.
			In this case, the feature description in the local area of these keypoints matches.
			%Two keypoints with associated feature descriptors, one in each image, are considered a corresponding pair if the feature descriptions are equal.
			A reasonable feature descriptor should be invariant to rotation, scale, illumination changes and noise.
			One of the descriptors that addresses these concerns is the \emph{Scale Invariant Feature Transform (SIFT)} introduced by~\cite{lowe1999object}.
			In the first step, the feature detection, SIFT identifies patterns that are distinctive from their neighborhood by finding edges, corners and blobs in the image.
			The second step transforms these patterns (in form of image patches) into a scale-invariant representation by convolving with multiple Difference of Gaussian (DoG) kernels at different scales of the image (via down-sampling).
			At this stage, one of the candidate scales is selected based on contrast and edge response.
			In this way, the same pattern in two different images at different scales will be transformed to the same scale in the feature space and therefore invariance is satisfied.
			In the third step, rotation invariance is achieved by computing a dominant orientation of the patch based on local gradients.
			The patch is then further subdivided into subregions where a histogram of gradients (HOG) is computed.
			All of this information is then encoded into a 128-dimensional vector describing a single feature.
			
			The extraction of these features works the same for every image, and the features descriptors can be compared directly to find correspondence candidates.
			Although SIFT has proven to be very robust, the computation of the features is expensive, especially the scale-invariant part.
			Subsequent works have proposed improved variants of SIFT such as PCA-SIFT (\cite{ke2004pca}) or SURF (\cite{bay2006surf}).
			Despite the successes of these methods in many applications, e.g. object recognition or panorama stitching, they rely on hand-crafted functions that are empirically derived.
			Furthermore, SIFT is defined based on very low-level image characteristics, which is contrary to the workings of the human brain, the most-sophisticated system for vision we know.
			
			As we will see in section~\ref{sec:learning_based_methods}, neural networks can be designed to automatically learn useful features without the need for manual intervention and tuning.
			These networks are built in an intuitive way.
			It is often easier and faster to train a neural network instead of manually adapting features to a new domain.

		\subsection{Structure and Motion from Two Images}
			Early works on SfM focused their study on the geometry between two camera views only.
			An important algorithm that emerged from these works is the \emph{The Eight Point Algorithm}, introduced by \cite{longuet1981}.
			It makes use of the epipolar geometry between a pair of views in order to compute relative translation and orientation of the cameras.
			The relationship between the two camera views is described by the so-called \emph{essential matrix}.
			It is defined as 
			\begin{equation}\label{eq:essential_matrix}
				\matr{E} = \left[ \vectr{t} \right]_\times \! \matr{R},
			\end{equation}
			where $\left[ \vectr{t} \right]_\times$ is the matrix that computes the cross-product between $\vectr{t}$ and an arbitrary vector.
			The essential matrix describes the relationship between corresponding points in the image planes of the two cameras.
			When $\matr{R}$ and $\vectr{t}$ are unknown, it is possible to compute the essential matrix using known corresponding points.
			The epipolar constraint is formulated as 
			\begin{equation}\label{eq:epipolar_constraint}
				\vectr{\hat{x}}_1^\top \matr{E} \, \vectr{\hat{x}}_0 = 0,
			\end{equation}
			where $\vectr{\hat{x}}_0$ and $\vectr{\hat{x}}_1$ are the corresponding points in image one and two respectively.
			The unknown elements of the essential matrix can be determined using this system of equations.
			It requires eight equations for eight elements of the matrix and one element has to be fixed because any scaling of the true matrix satisfies the epipolar constraint.
			After estimating $\matr{E}$, the rotation and translation can be uniquely recovered from it up to an unknown scaling of the translation.
			
			In practice, there are a few problems that need to be addressed.
			First, the corresponding points are usually not known and must be found using a feature detection and matching technique (SfM step 1), e.g. SIFT.
			This then can lead to outliers, which are incorrectly assigned correspondences.
			In this case, {RANSAC} (\cite{fischler1981random}) is applied to select the best eight points among many possible choices.
			Second, the selected coordinates are usually not exact and contain noise that lead to a rank three estimate of $\matr{E}$ instead of rank two.
			The singular value decomposition is used to discard the component with the smallest singular value.
			Third, in some cases the camera calibration matrices $\matr{K}_0$ and $\matr{K}_1$ are unknown.
			This means that the normalized coordinates $\vectr{\hat{x}}_j = \matr{K}_j^{-1} \vectr{x}_j$ are no longer available and only the image coordinates $\vectr{x}_j$ are known.
			This results in the so called \emph{fundamental matrix} $\matr{F}$ in the epipolar constraint:
			\begin{equation}\label{eq:fundamental_matrix_epipolar_constraint}
				\vectr{\hat{x}}_1^\top \matr{E} \vectr{\hat{x}}_0 = 
				\vectr{x}_1^\top \matr{K}_1^{-\top} \matr{E} \matr{K}_0^{-1} \vectr{x}_0 = 
				\vectr{x}_1^\top \matr{F} \vectr{x}_0 = 0.
			\end{equation}
			The same way as for the essential matrix, $\matr{F}$ can be computed using the Eight Point Algorithm although the fundamental matrix is less useful than the essential matrix because it is not anymore possible to uniquely recover $\matr{R}$ and $\matr{t}$ in the uncalibrated case.
			Nonetheless, $\matr{F}$ defines the epipolar constraint and therefore can be used to find more correspondences in the image pair.
			
			\todo{3D reconstruction from Essential matrix via triangulation}
			%The Eight Point Algorithm depends on a selection of pairs of coordinates, each pair representing the same 3D point in the two images respectively.
			%A manual selection of these correspondences is unpractical, hence the need for an automated system arises. 
			%\todo{numerical issues, 5 point algorithm}
			

			
		\subsection{Multi-View Structure from Motion}
		%\subsection{The Factorization Method}
			So far, we have seen a method for estimating the relative camera pose \todo{and structure} based on two images by solving the epipolar constraint with matched feature points.
			The next step is the reconstruction from multiple camera views.
			The factorization method can be used to recover motion and 3D shape simultaneously for all cameras (step 2 and 3).
			A first method was introduced by \cite{tomasi1992factorization} that works with an orthographic camera model.
			It uses the so called \emph{measurement matrix} $\matr{W} \in \R^{2n \times m}$ which is defined as
			\begin{equation}\label{eq:measurement_matrix}
				\matr{W} =
				\begin{bmatrix}
					x_{11} & \cdots & x_{1m} \\ 
					\vdots & \ddots & \vdots \\ 
					x_{n1} & \cdots & x_{nm} \\ 
					y_{11} & \cdots & y_{1m} \\ 
					\vdots & \ddots & \vdots \\ 
					y_{n1} & \cdots & y_{nm}
				\end{bmatrix}. 
			\end{equation}
			It contains the 2D coordinates of the $m$ orthographically projected 3D points for each of the $n$ cameras.
			Under the assumption that the origin of global coordinate system is located at the centroid of the point cloud, the matrix $W$ can be factorized into
			\begin{equation}\label{eq:factorization_method}
				\matr{W} = \matr{M} \matr{S},
			\end{equation}
			where $\matr{M} \in \R^{2n \times 3}$ contains the $2 \times 3$ projection matrices of each camera and $\matr{S} \in \R^{3 \times m}$  are the points of the observed 3D shape.
			As described by \cite{tomasi1992factorization}, this factorization can be achieved using the singular value decomposition (SVD) as $\matr{W} = \matr{U} \matr{\varSigma} \matr{V}^\top$ and deriving $\matr{M}$ and $\matr{S}$ from $\matr{U}$, $\matr{V}$ and $\matr{\varSigma}$.
			\todo{Add more details? Less details?}
			
			This simple factorization method has some disadvantages.
			First, it can not be applied to the commonly used perspective projection model, although it is approximated by the orthographic model for distant objects.
			One reason for this is the ambiguity that comes with the projective camera model.
			Applying a projective distortion $\matr{Q} \in \R^{4 \times 4}$ to the scene and inverting it by multiplying the inverse transformation to the camera matrix, the projection is exactly the same, i.e., 
			\begin{equation}\label{eq:projective_ambiguity}
				\matr{P} \vectr{x} = (\matr{P} \matr{Q}^{-1}) (\matr{Q} \vectr{x}).
			\end{equation}
			Without any constraints on the projection matrices, the factorization would be ambiguous.
			
			The works of \cite{sturm1996factorization} and \cite{christy1996euclidean} have extended the factorization method for perspective projection using an iterative process by first recovering the projective depth using the fundamental matrices between successive pairs of images.
			Second, in order for the factorization method to work, all points of the shape must be visible in every frame, i.e., to obtain the measurement matrix one needs to find the correspondences across all frames.
			This is a serious limitation makes the factorization method less usable in practice.
			
		%\subsection{Bundle Adjustment} 
			An alternative method to solve multi-view SfM is the \emph{bundle adjustment} technique.
			It is the most general technique for simultaneously recovering 3D shape and camera pose, but it is also computationally expensive.
			The bundle adjustment technique aims to minimize the re-projection error of the unknown 3D points and camera matrices.
			The re-projection error is formulated as the euclidean distance between the known image coordinates and the projection of the unknown points using the unknown camera matrices.
			
			More formally, let 
			$\left \lbrace \matr{C}_j \right \rbrace_{j = 1}^{m}$ 
			be the (unknown) camera matrices that encode location, rotation and intrinsic parameters, let 
			$\left \lbrace \vectr{p}_i \right \rbrace_{i = 1}^{n}$ 
			denote the (unknown) 3D points and let $\vectr{x}_{ij}$ be the (known) observed 2D coordinates of the point $\vectr{p}_i$ in camera $j$.
			Then, the objective of bundle adjustment is defined as the optimization problem 
			\begin{equation}\label{eq:bundle_adjustment}
				\begin{aligned}
					& \underset{\left\lbrace \vectr{p}_i \right\rbrace, \left\lbrace \matr{C}_j \right\rbrace}{\text{min}}
					& & \sum_{i = 1}^{n} \sum_{j = 1}^{m} v(i, j) \left\| P(\matr{C}_j, \vectr{p}_i) - \vectr{x}_{ij} \right\|_2 ,
				\end{aligned}
			\end{equation}
			where $v(i, j)$ denotes the visibility of point $i$ in camera $j$ and $P$ is the projection operation with homogeneous division.
			This optimization problem is non-convex and as explained by \cite{survey2017} na\"ive optimization algorithms achieve only a poor local minimum.
			One approach to find a better local minimum is to initialize using an other SfM algorithm such as the factorization method.
			Since bundle adjustment is very computationally expensive for many cameras, it is often used as a refinement step.
			
			
	\section{Learning-Based Methods}
	\label{sec:learning_based_methods}
	
		Recently, it has been shown that Convolutional Neural Networks (CNN) are very effective at detecting and extracting features in images. 
		Many computer vision problems like image classification have benefited from the newly introduced CNN architectures like AlexNet (\cite{krizhevsky2012imagenet}).
		A CNN applies a set of convolution filters to the input image, followed by a non-linear operation that acts as a thresholding. 
		Patterns in the input that ``match" the filters have a high response (activation), and these activations will again be convolved with new filters, layer after layer.
		The key is that the convolution filters are not fixed, but they are learned through many examples of data.
		Due to the limited kernel size of the convolutions, the receptive field increases with each layer, representing a growing abstraction level that is key to understanding complex spatial relationships in images.
		Network architectures like VGGNet (\cite{simonyan2014very}), GoogLeNet (\cite{szegedy2015going}) or ResNet (\cite{he2016deep}) have demonstrated that greater depth (more layers) has a positive impact on the learned features and performance for image classification and other tasks.
	
		\subsection{Learning Camera Motion}
	
			The PoseNet, as proposed by \cite{kendall2015posenet}, is a CNN that performs regression for the location and orientation of the camera given a single image as input.
			It outputs a 7D vector that describes the camera pose $\vectr{p} = [\vectr{x}, \vectr{q}]^\top$ with a location $\vectr{x}$ and orientation $\vectr{q}$ as a quaternion.
			%They simultaneously learn the quantities with the euclidean loss.
	%		\begin{equation}
	%			\mathcal{L}(I) = 
	%			\left\|
	%				\hat{x} - x 
	%			\right\|_2 
	%			+ \beta 
	%			\left\| 
	%				\hat{q} - \frac{q}{\left\| q \right\|} 
	%			\right\|_2,
	%		\end{equation}
	%		where $\beta$ is a parameter to balance the expected error of pose and orientation.
			The architecture is a modified GoogleNet with 23 layers which was trained on a classification task.
			They replace the last layers to perform regression instead of classification.
			
			Because CNNs require a large amount of training data, the authors apply transfer learning by pre-training the network on a different task with large datasets such as \emph{ImageNet} and \emph{Places}.
			Pose regression is performed using the pre-trained network and their own dataset called \emph{Cambridge Landmarks} with five scenes of camera motion in large scale outdoor areas for which ground truth pose is available.
			A challenge that comes with this dataset is the clutter in form of moving pedestrians and cars.
			Also, due to the long trajectories, weather and lighting conditions change a lot.
			This is contrary to the aforementioned classic SfM approaches where it is always assumed that the scene is static, without moving objects.
			
			In their experiments they find that the system is robust to large spatial distance between camera samples.
			Also, from the visualization of the features they observe that the network does not only produce high response outputs for high level features, but it is also sensitive to large textureless regions where {SIFT}-based approaches typically fail.
			
			For the most part, the objective of PoseNet is very different from the objective of this thesis.
			It estimates the camera parameters only based on one input image and in an absolute coordinate system.
			This means that network is specifically trained for a small number of scenes, e.g., the Cambridge landmarks, and it is not intended to generalize to other scenes and settings. 
			Nonetheless, the work of \citeauthor{kendall2015posenet} shows that transfer-learning can be used for pose estimation when a large labeled dataset for training is not available and that the pre-trained network can aid to learn pose information despite being trained for pose-invariant outputs.
			
		%\subsection{05/2017 - Relative Camera Pose Estimation Using Convolutional Neural Networks}
		
			\cite{melekhov2017poseCNN} propose a CNN architecture to estimate the relative pose between two camera images.
			Similar to PoseNet \cite{kendall2015posenet}, they estimate the 7D pose vector with a network composed of two main parts.
			The first is the \emph{representation part}.
			It consists of two identical branches with shared weights of five convolutional layers with ReLUs and Spatial Pyramid Pooling (SPP) at the end.
			Each branch processes one of the input images.
			The second part is the \emph{regression part}.
			Regression is performed on the output of the representation part using two fully-connected layers to estimate the relative pose.
			The weights for the Siamese network are initialized from a pre-trained AlexNet on the \emph{ImageNet} and \emph{Places} datasets.
			\citeauthor{melekhov2017poseCNN} apply SPP in the representation part in order to overcome the limitation in the input size due to the fully-connected layers.
			This allows for a larger size of input images for the network to be able to extract more structural information that helps reconstructing the pose.
			The dataset used for training is a crowd-sourced image collection with ground truth pose from a SfM pipeline based on local feature matching.
			For testing and comparison with other works they use the \emph{DTU} dataset where the exact pose is obtained from a robotic arm that moves the camera.
			
			The difference to PoseNet is that the estimated camera pose is relative, and the aim is to learn a representation for motion between frames instead of absolute localization.
			This is a more general approach, with a clear separation of concerns, i.e., the feature extraction (representation part) and pose estimation (regression part).
			It could be directly applied for Visual Odometry by computing the relative pose between subsequent pairs of images in the video. 
			In fact, \cite{mohanty2016deepvo} have exactly followed this idea.
			Their model has a very similar architecture as \citeauthor{melekhov2017poseCNN}.
			What stands out is their experiment where prior knowledge is supplied to the CNN by an additional input of features from the FAST corner detector (\cite{rosten2006machine}).
			They find that the CNN learns to extract similar features to FAST that work better compared to their initial experiments using a pre-trained AlexNet.
			
			\cite{wang2017deepvo} add a new component to the past deep learning approaches for VO, the recurrent neural network (RNN).
			The RNN differs from a regular neural network in the sense that it is designed to learn dependencies over a variable amount of time.
			This makes it suitable for problems that involve a sequence of inputs with temporal dependency, as in a video.
			The idea is that the information learned in the past (e.g. from earlier video frames) provides the \emph{context} for the present input.
			\citeauthor{wang2017deepvo} also use a pre-trained CNN architecture for feature extraction, followed by the RNN for pose regression.
			However, as opposed to previous works, the CNN is not initialized with weights from motion-invariant tasks. 
			It is a modified version of FlowNet (\cite{dosovitskiy2015flownet}) pre-trained on the synthetic \emph{Flying Chairs} dataset.
			Although this data does not include camera motion, \citeauthor{wang2017deepvo} show that it still serves as a good initialization for the VO problem.
			Their paper was released concurrently to this thesis and both share the same ideas, hence this thesis can be seen as a study and supplement to the original work.
			
			There is an alternative approach by the same authors (\cite{clark2017vinet}) that tackles the problem of Visual-\emph{Inertial} Odometry.
			The interesting part of their architecture called VINet is that they provide pose measurements from an inertial measurement unit (IMU) as input to the network, in addition to the video.
			The IMU is a physical device that consists of accelerometers and gyroscopes which are used to derive position and orientation of the device.
			The measurement accuracy of the IMU suffers from accumulated error over time, so-called ``drift''.
			Therefore the input to VINet is not the true camera pose, but a noisy measurement of it.
			\citeauthor{clark2017vinet} use two separate RNNs, one for the visual signal (the video input) and one for the pose stream (from the IMU).
			The idea behind this setup is that the combined input of both the visual features and the noisy pose measurements improve the networks ability to output a more accurate ego-motion estimate.
			In theory, the additional pose input should help the network handle scene dynamics and the recovery of scale.
			Although the work demonstrates that RNNs are useful for combining information of multiple streams of data, the aim of this thesis is pure VO with only the video input as resource for ego-motion.
			
			
			\todo{Summarize, trend etc.}
			
			
	%\subsection{04/2016 - 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction}
		
		%\cite{choy20163d}
		
		
		\subsection{Learning Structure and Motion}
			The previous subsection has given an overview of recent works for learning ego-motion from video.
			In theory, all the presented networks have to learn a weak representation of 3D structure even though they are never asked to output depth.
			This is because motion and depth are two highly coupled quantities that both contribute to the optical flow in the observed image sequence.
			
			\cite{SFMNET} implement a deep learning approach to SfM that supports various types of supervision. 
			Their architecture consists of two subnetworks, one for motion and one for structure.
			The former takes a pair of consecutive video frames as input and computes a fixed number of segmentation masks for moving objects. 
			Using these masks, each pixel is assigned to an object-specific transformation by a learned rotation and a translation.
			In addition, camera rotation and translation are regressed using the features from intermediate layers.
			The second subnetwork estimates depth from a single input image (the left image of the pair in the motion network). 
			The output is a cloud of 3D points, one for each pixel coordinate in the input image.
			
			In total, the network estimates relative camera motion, per-object transformations with motion masks, and a point cloud.
			Using the camera and object transforms, the point cloud can be re-projected into the two camera views to obtain the optical flow.
			This then presents two options for supervision.
			One is self-supervision with a brightness-constancy criterion between the two input frames using the optical flow.
			The other option is to supervise all components individually, i.e. camera motion, depth and optical flow.
			Their results show typical failure cases in the self-supervised setting where shadows and occlusions break the brightness-consistency criterion, which hinders the network to properly separate object and camera motion.
			However, the self-supervised approach still opens the possibility to use it as an initialization process for a supervised training on smaller datasets.
		
			In a concurrent work, \cite{zhou2017unsupervised} share a similar concept of camera pose and depth estimation.
			
			----------------
			Both works share a similar concept.
			However, \cite{zhou2017unsupervised} focus on unsupervised training and do not explicitly model scene dynamics.
			Similar to SfM-Net, their architecture consists of two jointly trained networks, one for learning depth from a single image (Depth-CNN), and another for learning camera motion from two frames (Pose-CNN).
			
			The key difference to SfM-Net is that they train with an image sequence $I_1, \dots, I_N$ with one image being the target image $I_t$.
			The task for the motion network is to synthesize the target image from two nearby views $I_{t - 1}$ and $I_{t + 1}$.
			\begin{itemize}
				\item \textbf{Depth-CNN}
				\\
				Takes the target image $I_t$ as input and predicts a per-pixel depth map $\hat{D}_t$.
				\item \textbf{Pose-CNN}
				\\
				Uses two source images $I_{t - 1}$ and $I_{t + 1}$ as input and predicts relative camera poses $\hat{T}_{t \rightarrow t - 1}$ and $\hat{T}_{t \rightarrow t + 1}$. 
				These transformations are then used to synthesize the target image.
			\end{itemize}
			Using both the estimated depth and pose matrix as well as the intrinsic matrix of the camera, the coordinate frame of the source images is warped to the coordinate frame of the target image.
			To transfer the color information from one frame to the other, bilinear interpolation is used.
			The general term for this is \emph{differentiable image warping}.
			
			When working with realistic data, it is important to make the model robust to non-static objects in the scenes, occlusions and non-Lambertian surfaces.
			To do this, the network is forced to additionally learn a per-pixel mask for each target-source pair.
			They add a regularization term to force the mask to have non-zero values.
			
			
		------------------------------------
		
		\iffalse
		
		\subsection{07/2016 - Unsupervised CNN for Single View Depth Estimation}
		The work of \cite{garg2016} implements an autoencoder on stereo pair images to predict depth from a single image.
		Their architecture consist of the following parts.
		\begin{itemize}
			\item \textbf{Encoder}
				\\
				Takes a single image as input. 
				At training time, this is the left image of a stereo pair.
				The output is the predicted disparity map (scaled inverse depth).
			\item \textbf{Decoder}
				\\
				The decoder is only used at training time.
				It takes two inputs: The predicted disparities from the encoder and the right image of the stereo pair.
				The right image is warped using the displacements and compared to the encoder input (left image) using the color constancy error (photometric error).
		\end{itemize}
		Since the stereo pair is rectified, the disparity is a displacement along the scanline of the images.
		Thus the decoder implements a simple geometric transformation that does not need to be learned.
		At test time, only the encoder network with a single image as input is used.
		
		As noted by the author, there are standard stereo algorithms that produce disparity maps.
		However, these methods can not deal with distortions such as lens flare, motion blur, shadows, etc.
		The idea is that a neural network could learn to deal with such problems that occur in natural images.
		
		The dataset they use is {KITTI} \cite{Geiger2012KITTI} which they augment by random crops, color channel scaling, and flipping the images.
		
	
	\subsection{04/2017 - SfM-Net}
		
		\cite{SFMNET} implement a deep learning approach to structure from motion. 
		Their architecture consists of two subnetworks:
		\begin{itemize}
			\item \textbf{Structure}
				\\
				Learns per-frame depth.
				The input is a single frame. 
				The output of the CNN is a cloud of 3D points, one for each pixel value in the input image.
			\item \textbf{Motion}
				\\
				The input is a pair of frames.
				The CNN computes a set of $K$ segmentation masks for moving objects. 
				Using these masks, each pixel is assigned to an object specific transformation given by a rotation and a translation.
				In addition, camera rotation and translation are computed using the features of the inner layers.
		\end{itemize}
		Given a pair of images, the forward operation of the network works as follows:
		\begin{enumerate}
			\item The structure network computes the point cloud for the first frame.
			\item The motion network computes the object transformations as well as the camera transformation using both frames.
			\item The point cloud is transformed using the learned object transformations and masks.
			\item The transformed 3D points are re-projected to 2D using the learned camera transformation between the two frames.
		\end{enumerate}
		The transformed point cloud corresponds to the depth of the second frame.
		Optical flow can be computed directly from the re-projected points.
		
		The authors of the paper propose various modes of supervision to evaluate the architecture and to handle ambiguities in the reconstruction due to the ill-posed problem:
		\begin{itemize}
		\item \textbf{Self-supervision}
			\\
			No ground truth is given.
			The loss is defined by the brightness constancy constraint of the second frame warped to the first frame using the predicted optical flow.
			For this mode, they use the {KITTI} 2012/15 datasets.
		\item \textbf{Depth}
			\\
			Ground truth is given in the form of depth for each pixel.
			This can be acquired for example by a Kinect sensor.
			They use the {RGB-D SLAM} dataset for ground truth depth.
			This helps to improve camera motion estimation. 
		\item \textbf{Camera motion}
			\\
			Camera motion is given as ground truth in form of a rotation and translation matrix.
			The relative transformation between predicted and ground truth transformation is 
		\item \textbf{Optical flow and object motion}
			\\
			They use this type of supervision with the MoSeg dataset which contains ground truth segmentation for each frame.
			This dataset contains more non-rigid body transformation.
			They evaluate the quality of the object motion mask by intersection over union (IoU).
		\end{itemize}
		
	\subsection{04/2017 - Unsupervised Learning of Depth and Ego-Motion from Video}
		
		This work from \cite{zhou2017unsupervised} is concurrent to the SfM-Net by \cite{SFMNET}.
		Both works share a similar concept.
		However, \cite{zhou2017unsupervised} focus on unsupervised training and do not explicitly model scene dynamics.
		Similar to SfM-Net, their architecture consists of two jointly trained networks, one for learning depth from a single image (Depth-CNN), and another for learning camera motion from two frames (Pose-CNN).
		
		The key difference to SfM-Net is that they train with an image sequence $I_1, \dots, I_N$ with one image being the target image $I_t$.
		The task for the motion network is to synthesize the target image from two nearby views $I_{t - 1}$ and $I_{t + 1}$.
		\begin{itemize}
			\item \textbf{Depth-CNN}
				\\
				Takes the target image $I_t$ as input and predicts a per-pixel depth map $\hat{D}_t$.
			\item \textbf{Pose-CNN}
				\\
				Uses two source images $I_{t - 1}$ and $I_{t + 1}$ as input and predicts relative camera poses $\hat{T}_{t \rightarrow t - 1}$ and $\hat{T}_{t \rightarrow t + 1}$. 
				These transformations are then used to synthesize the target image.
		\end{itemize}
		Using both the estimated depth and pose matrix as well as the intrinsic matrix of the camera, the coordinate frame of the source images is warped to the coordinate frame of the target image.
		To transfer the color information from one frame to the other, bilinear interpolation is used.
		The general term for this is \emph{differentiable image warping}.
		
		When working with realistic data, it is important to make the model robust to non-static objects in the scenes, occlusions and non-Lambertian surfaces.
		To do this, the network is forced to additionally learn a per-pixel mask for each target-source pair.
		They add a regularization term to force the mask to have non-zero values.
		
	\subsection{04/2017 - DeMoN: Depth and Motion Network for Learning Monocular Stereo}
		
		\cite{ummenhofer2016demon} propose a network architecture to learn depth, motion as well as optical flow and surface normals from two images.
		The network architecture consists of three subnetworks in sequence:
		\begin{itemize}
			\item \textbf{Bootstrap network}
				\\
				Consists of a pair of encoder-decoder networks.
				The first encoder-decoder predicts optical flow and its confidence.
				The second encoder-decoder takes the previously computed optical flow and confidence, as well as the original image pair and a warp of the second image using the estimated flow.
				In addition, there are three fully connected layers predicting motion.
				By taking the image pair as input, the network can make use of motion parallax.
			\item \textbf{Iterative network}
				\\
				The architecture is identical to the bootstrap network, but takes additional inputs.
				It is applied iteratively with the idea to improve previous estimates of depth, normals and motion.
				The network is trained using four iterations.
			\item \textbf{Refinement network}
				\\
				Up-samples the low resolution predictions to the full image resolution.
		\end{itemize}
		Motion is parameterized using an axis-angle representation and depth is estimated by its inverse value $1 / z$ in order to represent points at infinity.
		For surface normals, motion and optical flow they use the euclidean loss and for depth L1.
		In addition to the point wise loss for depth, they define a scale invariant gradient loss that penalizes relative depth errors between neighboring pixels.
		
		They evaluate the performance of the network on various datasets: {SUN3D}, {RGB-D SLAM}, {MVS}, {Scenes11}, and {NYUv2}.
		The results show that estimating normals and flow in addition to depth and motion slightly improves the performance of the system.
		Compared to works for depth from a single image, their results demonstrate that the network learns to use motion parallax from the stereo images to generalize better to scenes that heavily differ from the examples seen during training.
		
		\fi
		
	
		
