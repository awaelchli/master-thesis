\chapter{Prior Work}
	
	\todo{Add text that introduces the chapter} \\
	\todo{Find good structure/order to present the different works} \\
	\todo{Better point out differences and similarities} \\
	
	\noindent
	Possible definition of structure from motion by \cite{survey2017}:
	
	\say{The structure from motion (SfM) problem in computer vision is the problem of recovering the three-dimensional (3D) structure of a stationary scene from a set of projective measurements, represented as a collection of two-dimensional (2D) images, via estimation of motion of the cameras corresponding to these images.}
	
	One can define the three basic steps of SfM as:
	\begin{enumerate}
		\item Feature detection, extraction and matching
		\item Camera motion estimation
		\item Recovery of 3D structure
	\end{enumerate}
	This chapter presents the prior work that focuses on one or multiple of these steps.
	
	\section{Feature-based methods SURF, ORB, SIFT}
	

	\section{The Eight Point Algorithm}
		The eight point algorithm, as introduced by \cite{longuet1981}, 
		The \emph{essential matrix} is defined as 
		\begin{equation}\label{eq:essential_matrix}
			E = \left[ t \right]_\times R,
		\end{equation}
		where $\left[ t \right]_\times$ is the matrix that computes the cross-product between $t$ and an arbitrary vector.
		It describes the relationship between corresponding points in the image planes of the two cameras.
		When $R$ and $t$ are unknown, it is possible to compute the essential matrix using known corresponding points.
		The epipolar constraint is formulated as 
		\begin{equation}\label{eq:epipolar_constraint}
			\hat{x}_1^\top E \hat{x}_0 = 0,
		\end{equation}
		where $\hat{x}_0$ and $\hat{x}_1$ are the corresponding points in image one and two respectively.
		The unknown elements of the essential matrix can be determined using this system of equations.
		It requires eight equations for eight elements of the matrix and one element has to be fixed because any scaling of the true matrix satisfies the epipolar constraint.
		After estimating $E$, the rotation and translation can be recovered from it up to some ambiguities.
		
		In practice, there are some problems that need to be addressed.
		First, the corresponding points are usually not known and must be found using a feature detection and matching technique (SfM step 1).
		This then can lead to outliers which are wrong correspondences.
		In this case, {RANSAC} is applied to select the best eight points among many possible choices.
		Second, the selected points are usually not exact and contain noise that lead to a rank three estimate of $E$ instead of rank two.
		The singular value decomposition is used to discard the component with the smallest singular value.
		\todo{numerical issues, 5 point algorithm}
		Third, in some cases the calibration matrices $K_0$ and $K_1$ are unknown.
		This means that the normalized coordinates are $\hat{x}_j = K_j^{-1} x_j$ are no longer available and only the image coordinates $x_j$ are known.
		This results in the so called \emph{fundamental matrix} $F$ in the epipolar constraint:
		\begin{equation}
			\hat{x}_1^\top E \hat{x}_0 = x_1^\top K_1^{-\top} E K_0^{-1} x_0 = x_1^\top F x_0 = 0.
		\end{equation}
		\todo{explain what F is used for}
		
	\section{Factorization Methods}
		The factorization method can be used to recover camera motion and 3D shape simultaneously (step 2 and 3).
		A first method was introduced by \cite{tomasi1992factorization} that works with an orthographic camera model.
		It uses the so called \emph{measurement matrix} $W \in \mathbb{R}^{2n \times m}$ which is defined as
		\begin{equation}\label{eq:measurement_matrix}
			W =
			\begin{bmatrix}
				x_{11} & \cdots & x_{1m} \\ 
				\vdots & \ddots & \vdots \\ 
				x_{n1} & \cdots & x_{nm} \\ 
				y_{11} & \cdots & y_{1m} \\ 
				\vdots & \ddots & \vdots \\ 
				y_{n1} & \cdots & y_{nm}
			\end{bmatrix}. 
		\end{equation}
		It contains the 2D coordinates of the $m$ orthographically projected 3D points for each of the $n$ cameras.
		Under the assumption that the origin of global coordinate system is located at the centroid of the point cloud, the matrix $W$ can be factorized into
		\begin{equation}\label{eq:factorization_method}
			W = MS,
		\end{equation}
		where $M \in \mathbb{R}^{2n \times 3}$ contains the $2 \times 3$ projection matrices of each camera and $S \in \mathbb{R}^{3 \times m}$  are the points of the observed 3D shape.
		As described by \cite{tomasi1992factorization}, this factorization can be achieved using the singular value decomposition (SVD) as $W = U \varSigma V^\top$ and deriving $M$ and $S$ from $U$, $V$ and $\varSigma$.
		\todo{Add more details? Less details?}
		
		This factorization method has some disadvantages.
		First, it can not be applied to the commonly used perspective projection model, although it is approximated by the orthographic model for distant objects.
		The works of \cite{sturm1996factorization} and \cite{christy1996euclidean} have extended the factorization method for perspective projection using an iterative process.
		Second, in order for the factorization method to work, all points of the shape must be visible in every frame, i.e. to obtain the measurement matrix one needs to find the correspondences across all frames.
		
	\section{Bundle Adjustment} 
		Another method to solve multi-view SfM is the method of \emph{bundle adjustment}.
		It is the most general technique for simultaneously recovering 3D shape and camera pose, but it is also computationally expensive.
		The bundle adjustment technique aims to minimize the re-projection error of the unknown 3D points and camera matrices.
		The re-projection error is formulated as the euclidean distance between the known image coordinates and the projection of the unknown points using the unknown camera matrices.
		
		More formally, let 
			$\left \lbrace C_j \right \rbrace_{j = 1}^{m}$ 
		be the (unknown) camera matrices that encode location, rotation and intrinsic parameters, let 
			$\left \lbrace p_i \right \rbrace_{i = 1}^{n}$ 
		denote the (unknown) 3D points and let $x_{ij}$ be the (known) observed 2D coordinates of the point $p_i$ in camera $j$.
		Then, the objective of bundle adjustment is defined as
		\begin{equation}\label{eq:bundle_adjustment}
			\begin{aligned}
				& \underset{\left\lbrace p_i \right\rbrace, \left\lbrace C_j \right\rbrace}{\text{min}}
				& & \sum_{i = 1}^{n} \sum_{j = 1}^{m} v(i, j) \left\| P(C_j, p_i) - x_{ij} \right\|_2 ,
			\end{aligned}
		\end{equation}
		where $v(i, j)$ denotes the visibility of point $i$ in camera $j$ and $P$ is the projection operation with homogeneous division.
		This optimization problem is non-convex and as explained by \cite{survey2017} na\"ive optimization algorithms achieve only a poor local minimum.
		One approach to find a better local minimum is to initialize using an other SfM algorithm such as the factorization method.
		
%	\section{Camera Location Estimation}
%		To estimate the location and orientation of the cameras, one needs to find the rotation matrices $R_i \in \mathbb{R}^{3 \times 3}$ and translations $t_i \in \mathbb{R}^{3}$ for each of the cameras.
%		
%		\todo{?? This might be the focus of the thesis}
%		
	
	\section{2015 - PoseNet}
		The PoseNet, as proposed by \cite{kendall2015posenet}, is a CNN that performs regression for the location and orientation of the camera given a single image as input.
		It outputs a 7D vector that describes the pose $p = [x, q]^\top$ as the camera location $x$ (3D) and orientation $q$ (4D quaternion).
		They simultaneously learn location and orientation with the euclidean loss
		\begin{equation}
			\mathcal{L}(I) = 
			\left\|
				\hat{x} - x 
			\right\|_2 
			+ \beta 
			\left\| 
				\hat{q} - \frac{q}{\left\| q \right\|} 
			\right\|_2,
		\end{equation}
		where $\beta$ is a parameter to balance the expected error of pose and orientation.
		The architecture is a modified \emph{GoogleNet} with 23 layers which was trained on a classification task.
		They replace the last layers to perform regression instead of classification.
		
		Because CNNs require a large amount of training data, the authors apply transfer learning by pretraining the network on a different task with large datasets such as \emph{ImageNet} and \emph{Places}.
		Pose regression is performed using the pre-trained network and their own dataset called \emph{Cambridge Landmarks} with five scenes of camera motion in large scale outdoor areas.
		For this limited dataset, ground truth pose is available.
		A challenge that comes with this dataset is the clutter in form of moving pedestrians and cars.
		Also, due to the long trajectories, weather and lighting conditions change a lot.
		
		In their experiments they find that the system is robust to large spatial distance between camera samples.
		Also, from the visualization of the features they observe that the network does not only produce high response outputs for high level features, but is also sensitive to large textureless regions where {SIFT}-based approaches typically fail.
		
		The paper demonstrates that transfer-learning can be used for pose estimation when a large labeled dataset for training is not available and that the pre-trained network can learn pose information despite being forced to produce pose-invariant outputs.
		\todo{connection to other work?}
		
		
	\section{07/2016 - Unsupervised CNN for Single View Depth Estimation}
		
		The work of \cite{garg2016} implements a autoencoder on stereo pair images to predict depth from a single image.
		Their architecture consist of the following parts.
		\begin{itemize}
			\item \textbf{Encoder}
				\\
				Takes a single image as input. 
				At training time, this is the left image of a stereo pair.
				The output is the predicted disparity map (scaled inverse depth).
			\item \textbf{Decoder}
				\\
				The decoder is only used at training time.
				It takes two inputs: The predicted disparities from the encoder and the right image of the stereo pair.
				The right image is warped using the displacements and compared to the encoder input (left image) using the color constancy error (photometric error).
		\end{itemize}
		Since the stereo pair is rectified, the disparity is a displacement along the scanline of the images.
		Thus the decoder implements a simple geometric transformation that does not need to be learned.
		At test time, only the encoder network with a single image as input is used.
		
		As noted by the author, there are standard stereo algorithms that produce disparity maps.
		However, these methods can not deal with distortions such as lens flare, motion blur, shadows, etc.
		The idea is that a neural network could learn to deal with such problems that occur in natural images.
		
		The dataset they use is {KITTI} \cite{Geiger2012KITTI} which they augment by random crops, color channel scaling, and flipping the images.
		
	
	\section{04/2017 - SfM-Net}
		
		\cite{SFMNET} implement a deep learning approach to structure from motion. 
		Their architecture consists of two subnetworks:
		\begin{itemize}
			\item \textbf{Structure}
				\\
				Learns per-frame depth.
				The input is a single frame. 
				The output of the CNN is a cloud of 3D points, one for each pixel value in the input image.
			\item \textbf{Motion}
				\\
				The input is a pair of frames.
				The CNN computes a set of $K$ segmentation masks for moving objects. 
				Using these masks, each pixel is assigned to an object specific transformation given by a rotation and a translation.
				In addition, camera rotation and translation are computed using the features of the inner layers.
		\end{itemize}
		Given a pair of images, the forward operation of the network works as follows:
		\begin{enumerate}
			\item The structure network computes the point cloud for the first frame.
			\item The motion network computes the object transformations as well as the camera transformation using both frames.
			\item The point cloud is transformed using the learned object transformations and masks.
			\item The transformed 3D points are re-projected to 2D using the learned camera transformation between the two frames.
		\end{enumerate}
		The transformed point cloud corresponds to the depth of the second frame.
		Optical flow can be computed directly from the re-projected points.
		
		The authors of the paper propose various modes of supervision to evaluate the architecture and to handle ambiguities in the reconstruction due to the ill-posed problem:
		\begin{itemize}
		\item \textbf{Self-supervision}
			\\
			No ground truth is given.
			The loss is defined by the brightness constancy constraint of the second frame warped to the first frame using the predicted optical flow.
			For this mode, they use the {KITTI} 2012/15 datasets.
		\item \textbf{Depth}
			\\
			Ground truth is given in the form of depth for each pixel.
			This can be acquired for example by a Kinect sensor.
			They use the {RGB-D SLAM} dataset for ground truth depth.
			This helps to improve camera motion estimation. 
		\item \textbf{Camera motion}
			\\
			Camera motion is given as ground truth in form of a rotation and translation matrix.
			The relative transformation between predicted and ground truth transformation is 
		\item \textbf{Optical flow and object motion}
			\\
			They use this type of supervision with the MoSeg dataset which contains ground truth segmentation for each frame.
			This dataset contains more non-rigid body transformation.
			They evaluate the quality of the object motion mask by intersection over union (IoU).
		\end{itemize}
		
	\section{04/2017 - Unsupervised Learning of Depth and Ego-Motion from Video}
		
		This work from \cite{zhou2017unsupervised} is concurrent to the SfM-Net by \cite{SFMNET}.
		Both works share a similar concept.
		However, \cite{zhou2017unsupervised} focus on unsupervised training and do not explicitly model scene dynamics.
		Similar to SfM-Net, their architecture consists of two jointly trained networks, one for learning depth from a single image (Depth-CNN), and another for learning camera motion from two frames (Pose-CNN).
		
		The key difference to SfM-Net is that they train with an image sequence $I_1, \dots, I_N$ with one image being the target image $I_t$.
		The task for the motion network is to synthesize the target image from two nearby views $I_{t - 1}$ and $I_{t + 1}$.
		\begin{itemize}
			\item \textbf{Depth-CNN}
				\\
				Takes the target image $I_t$ as input and predicts a per-pixel depth map $\hat{D}_t$.
			\item \textbf{Pose-CNN}
				\\
				Uses two source images $I_{t - 1}$ and $I_{t + 1}$ as input and predicts relative camera poses $\hat{T}_{t \rightarrow t - 1}$ and $\hat{T}_{t \rightarrow t + 1}$. 
				These transformations are then used to synthesize the target image.
		\end{itemize}
		Using both the estimated depth and pose matrix as well as the intrinsic matrix of the camera, the coordinate frame of the source images is warped to the coordinate frame of the target image.
		To transfer the color information from one frame to the other, bilinear interpolation is used.
		The general term for this is \emph{differentiable image warping}.
		
		When working with realistic data, it is important to make the model robust to non-static objects in the scenes, occlusions and non-Lambertian surfaces.
		To do this, the network is forced to additionally learn a per-pixel mask for each target-source pair.
		They add a regularization term to force the mask to have non-zero values.
		
	\section{05/2017 - Relative Camera Pose Estimation Using Convolutional Neural Networks}
	
		\cite{melekhov2017poseCNN} propose a CNN architecture to estimate the relative pose between two cameras.
		Similar to PoseNet \cite{kendall2015posenet}, they estimate the 7D vector of pose by a quaternion and a translation vector.
		Their network is composed of two main parts.
		\begin{itemize}
			\item \textbf{Representation part}
				\\
				The inputs are the two camera images. 
				This part has two identical branches with shared weights of five convolutional layers with ReLUs and spatial pyramid pooling at the end.
				Each branch processes one of the input images.
			\item \textbf{Regression part}
				\\
				Regression is performed on the output of the representation part using two fully-connected layers to estimate the relative pose.
		\end{itemize}
		The weights for the Siamese network are initialized from a pre-trained AlexNet on the \emph{ImageNet} and \emph{Places} datasets.
		In order to overcome the limitation in the input size due to the fully-connected layers, \cite{melekhov2017poseCNN} apply spatial pyramid pooling (SPP) in the representation part.
		This allows for a larger size of input images for the network to be able to extract more structural information that helps reconstructing the pose.
		
		The dataset used for training is a crowd-sourced image collection with ground truth pose from a structure from motion pipeline based on local feature matching.
		For testing and comparison with other works they use the {DTU} dataset where the exact pose is obtained from a robotic arm that moves the camera.
		\todo{incomplete summary}
		
